## 카프카 기본 구조
• 카프카 내부에 데이터가 저장되는 파티션의 동작은 FIFO 방식의 큐 자료구조와 유사함
• 프로듀서 -> 토픽 -> 컨슈머
    • 프로듀서 : 큐에 데이터를 보내는 것
    • 토픽 : 구분하고자 하는 데이터의 구분에 따라서 토픽을 새로 만들고 운영
            토픽에서는 한개 이상의 파티션을 가지게 됨
            특정 메시지는 내부 로직에 따라서 여러 파티션중 하나에 데이터가 적재됨
            파티션 구조는 큐 구조와 유사
    • 컨슈머 : 큐에서 데이터를 가져가는 것 (처음부터 차례대로 가져감(FIFO))
              컨슈머가 파티션에서 데이터를 가져가더라도 데이터가 삭제되지 않음
              특정 컨슈머가 어떤 데이터 가져갔는지 커밋을 통해서 알 수 있음

• 카프카의 특징
    • 높은 처리량 : 프로듀서가 브로커로 데이터를 보낼 때와 컨슈머가 브로커로부터 데이터를 받을 때 모두 묶어서 전송
                  많은 양의 데이터를 묶음 단위로 처리하는 배치로 빠르게 처리할 수 있기 때문에 대용량의 실시간 로그 데이터를 처리하는데에 적합함
                  파티션 단위를 통해 동일 목적의 데이털르 여러 파티션에 분배하고 데이터를 병렬 처리할 수 있음
                  파티션 개수만큼 컨슈머 개수를 늘려서 동일 시간당 데이터 처리량을 늘리는 것
                  * 스케일 아웃 : 인스턴스 개수를 늘리는 것 (여기서는 컨슈머 개수)

    • 확장성     : 데이터가 적을 때는 카프카 클러스터의 브로커를 최소한의 개수로 운영하다가 데이터가 많아지면
                  클러스터의 브로커 개수를 자연스럽게 늘려 스케일 아웃할 수 있음
                  반대로 데이터 개수가 적어지고 추가 서버들이 더는 필요없어지면 브로커 개수를 줄여 스케일 인 함
                  카프카의 스케일 아웃, 스케일 인 과정은 클러스터의 무중단 운영을 지원하므로 24시간 데이터 처리하는
                  은행, 커머스같은 비즈니스 모델에서도 안정적인 운영이 가능함

    • 영속성     : 영속성이란 데이터를 생성한 프로그램이 종료되더라도 사라지지 않은 데이터의 특성을 뜻함
                  카프카는 다른 메시징 플랫폼과 다르게 전송받은 데이터를 메모리에 저장하지 않고 파일 시스템에 저장함
                  운영체제에서 파일 I/O성능 향상을 위해 페이지 캐시 영역을 메모리에 따로 생성하여 사용함
                  페이지 캐시 메모리 영역을 사용하여 한번 읽은 파일 내용은 메모리에 저장시켰다가 다시 사용하는 방식으로
                  카프카가 파일 시스템에 저장하고 데이터를 저장, 전송하더라도 처리량이 높음
                  디스크 기반의 파일 시스템을 활용하여 브로커 애플리케이션이 장애가 나더라도 프로세스를 재시작하여 안전하게 데이터 처리 가능

    • 고가용성   : 3개 이상의 서버들로 운영하여 일부 서버에 장애가 발생하더라도 무중단으로 안전하고 지속적으로 데이터 처리가능
                  클러스터로 이루어진 카프카는 데이터의 복제를 통해 고가용성의 특징을 가짐
                  프로듀서로 전송받은 데이터를 여러 브로커 중 1대의 브로커에만 저장하는 것이 아니라 또 다른 브로커에도 저장하는 것
                  한 브로커에 장애가 발생하더라도 복제된 데이터가 나머지 브로커에 저장되어 있으므로 저장된 데이터를 기준으로, 지속적으로 데이터 처리가 가능
                  서버를 직접 운영하는(on-premise) 환경의 서버 랙 또는 퍼블릭 클라우드의 리전 다위 장애에도 데이터를 안전하게 복제하는 브로커 옵션이 있음

## 카프카 브로커 • 클러스터 • 주키퍼

• 카프카 브로커는 카프카 클라이언트와 데이터를 주고받기 위해 사용하는 주체이자,
  데이터를 분산 저장하여 장애가 발생하더라도 하나하나가 프로세스로써 안전하게 사용할 수 있도록 도와주는 애플리케이션이다.
  하나의 서버에는 한 개의 카프카 브로커 프로세스가 실행된다. 하지만 데이터를 안전하게 보관하고 처리하기 위해서
  보통 3대 이상의 브로커 서버를 1개의 클러스터로 묶어서 운영한다. 카프카 클러스터로 묶인 브로커들은 프로듀서가 보낸
  데이터를 안전하게 분산 저장하고 복제하는 역할을 수행한다
  * 카프카 프로듀서 : 메시지를 생성해서 카프카의 토픽으로 메시지를 보내는 역할을 하는 애플리케이션, 서버 등
                   주요 기능으로서 각각의 메시지를 토픽 파티션에 매핑하고 파티션의 리더에 요청을 보내는 것

• 카프카 클러스터를 수행하기 위해서 주키퍼가 필요함
  주키퍼의 서로 다른 znode에 클러스터를 지정하면 됨
  root znode에 각 클러스터별 znode를 생성하고 클러스터 실행시 root가 아닌 하위 znode로 설ㅈ어
  카프카 3.0부터는 주키퍼가 없어도 클러스터 동작 가능

• 카파 아키텍쳐의 활용
    • 로그는 배치 데이터를 스트림으로 표현하기에 적합, 일반적으로 데이터 플랫폼에서 배치 데이터를 표현할 때는 각 시점(시점별, 일자별)의 전체 데이터를 백업한 스냅샷 데이터를 뜻함
      그러나 배치 데이터를 로그로 표현할 때는 각 시점의 배치 데이터의 표현 기록(change log)을 시간 순서대로 기록함으로써 각 시점의 모든 스냅샷 데이털르 저장하지 않고도 배치데이터 표현 가능
      * 변환 기록 로그에 timestamp가 기록되어야 됨

    • 배치 데이터 : 한정된(bounded) 데이터 처리
                  대규모 배치 데이터를 위한 분산 처리 수행
                  분, 시간, 일단위 처리를 위한 지연 발생
                  복잡한 키 조인 수행

    • 스트림 데이터 : 무한(unbounded) 데이터 처리
                    지속적으로 들어오는 데이터를 위한 분산 처리 수행
                    분 단위 이하 지연 발생
                    단순한 키 조인 수행

    • Materialized View로 기존에 하둡으로 사용하던 것을 처리해줌

    • 스트림 데이터를 배치 데이터로 사용하는 방법은 로그에 시간을 남기는 것, 로그에 남겨진 시간을 기준으로 데이터를 처리하면 스트림으로 적재된 데이터도 배치로 처리할 수 있게 됨
      카프카는 로그에 시간(timestamp)을 남기기 때문에 이런 방식의 처리가 가능

• 브로커의 역할 : 컨트롤러, 데이터 삭제
    • 컨트롤러
        • 클러스터의 다수 브로커 중 한 대가 컨트롤러의 역할을 한다. 컨트롤러는 다른 브로커들의 상태를 체크하고 브로커가
          클러스터로 빠지는 경우(이슈 발생시) 해당 브로커에 존재하는 리더 파티션을 재분배한다. 카프카는 지속적으로 데이터를 처리해야 하므로
          브로커의 상태가 비정상이라면 빠르게 클러스터에서 빼내는 것이 중요하다. 만약 컨트롤러 역할을 하는 브로커에 장애가 생기면
          다른 브로커가 컨트롤러 역할을 한다

    • 데이터 삭제
        • 카프카는 다른 메시징 플랫폼과 다르게 컨슈머가 데이터를 가져가더라도 토픽의 데이터는 삭제되지 않는다
          또한, 컨슈머나 프로듀서가 데이터 삭제를 요청할 수도 없고, 오직 브로커만이 데이터를 삭제할 수 있다
          데이터 삭제는 파일 단위로 이루어지는데 이 단위를 '로그 세그먼트(log segment)'라고 부른다
          이 세그먼트에는 다수의 데이터가 들어 있기 때문에 일반적인 데이터베이스처럼 특정 데이터를 선별해서 삭제할 수 없다

    • 컨슈머 오프셋 저장 (자동 생성)
        • 컨슈머 그룹은 토픽이 특정 파티션으로부터 데이터를 가져가서 처리하고 이 파티션의 어느 레코드까지 가져갔는지 확인하기 위해 오프셋을 커밋함
          커밋한 오프셋은 _conusmer_offsets 토픽에 저장, 여기에 저장된 오프셋을 토대로 컨슈머 그룹은 다음 레코드를 가져가서 처리함

    • 그룹 코디네이터
        • 코디네이터는 컨슈머 그룹의 상태를 체크하고 파티션을 컨슈머와 매칭되도록 분배하는 역할
          컨슈머가 컨슈머 그룹에서 빠지면 매칭되지 않은 파티션을 정상 동작하는 컨슈머로 할당하여 끊임없이 데이터가 처리되도록 도와줌
          파티션을 컨슈머로 재할당하는 과정을 '리밸런스' 라고 부름

    • 데이터 저장
        • 카프카를 실행할 때 config/server.properties의 log.dir 옵션에 정의한 디렉토리에 데이터를 저장(토픽 이름과 파티션 번호의 조합으로 하위 디렉토리 생성)
          hello.kafka-0: hello.kafka 토픽의 0번 파티션에 존재하는 데이터를 확인할 수 있다, ~log에는 메시지와 메타데이터를 저장
          ~index는 메시지의 오프셋(offset)을 인덱싱한 정보를 담은 파일, ~timeindex 파일에는 메시지에 포함된 timestamp값을 기준으로 인덱싱한 정보가 담김
          * log와 세그먼트 : log.segment.bytes (byte단위의 최대 세드먼트 크기 지정, 기본값은 1GB)
                           log.roll.ms(hours) (세그먼트가 신규 생성된 이후 다음파일로 넘어가는 주기. 기본값은 7일)
                           오프셋은 레코 드의 고유한 번호
                           프로듀서가 레코드를 만들어서 브로커에 넘기게 되면 특정 파티션중 하나에 저장이 되어있는데, 고유한 번호가 새로 지정됨
          가장 마지막 세그먼트 파일(쓰기가 일어나고 있는 파일)을 액티브 세그먼트라고 부름, 액티브 세그먼트는 브로커의 삭제 대상에 포함되지 않음
          세그먼트는 retention 옵션에 따라 삭제 대상으로 지정됨

    • 세그먼트와 삭제 주기(cleanup.policy = delete)
        • retention.ms(minute, hours) : 세그먼트를 보유할 최대 기간 (기본 값은 7일, 일반적으로 3일)
          retention.bytes : 파티션당 로그 적재 바이트 값, 기본값은 -1 (지정하지 않음)
          log.retention.check.interval.ms : 세그먼트가 삭제 영역에 들어왔는지 확인하는 간격(기본 값은 5분)
          * 디스크 용량에 따른 기간을 정해야됨

        • 삭제 : 카프카에서 데이터는 세그먼트 단위로 삭제가 발생하기 때문에 로그 단위(레코드 단위)로 개별 삭제는 불가능함
          로그의 메시지 키, 메시지 값, 오프셋, 헤더 등 이미 적재된 데이터에 대해서 수정 또한 불가능 하기 대문에 데이터를 적재할 때(프로듀서)
          또는 데이터를 사용할 때(컨슈머) 데이터를 검증하는 것이 좋음

        • cleanup.policy = compact : 압축이란 메시지 키 별로 해당 메시지 키의 레코드 중 오래된 데이터를 삭제하는 정책
          삭제(delete)정책과 다르게 일부 레코드만 삭제가 될 수 있음, 압축은 액티브 세그먼트를 제외한 데이터가 대상
          • 테일/헤드 영역, 클린/더티 로그
            • 테일 영역 : 압축 정책에 의해 압축이 완료된 레코드들, 클린(clean)로그 라고도 부름 (중복 메시지 키가 없음)
            • 헤드 영역 : 압축 정책이 되기 전 레코드들, 더티(dirty)로그 라고도 부름 (중복된 메시지 키가 있음)

        • min.cleanable.dirty.ratio
            • 데이터의 압축 시작 시점은 min.cleanable.dirty.ratio 옵션값을 따름, 옵션값은 액티브 세그먼트를 제외한 세그먼트에 남아있는
              테일 영역과 헤드 영역의 레코드 개수의 비율을 뜻함
              ex) 옵션값 0.5(50%)일때 테일 영역과 헤드 영역의 레코드 개수가 동일할 경우 압축이 실행
              0.9(90%)와 같이 크게 설정하면 한번 압축을 할 때 많은 데이터가 줄어드므로 압축 효과가 좋음, 그러나 0.9 비율이 될때까지 용량을 차지하므로 용량 효율은 저하
              0.1(10%)과 같이 작게 설정하면 압축이 자주 일어나서 가장 최신의 데이터를 유지하지만, 압축이 자주 발생하기 때문에 브로커에 부담을 줄 수 있음

    • 복제(Replication)
        • 데이터 복제(Replication)는 카프카를 장애 허용 시스템(default tolrerant system)으로 동작하도록 하는 원동력임
          복제의 이유는 클러스터로 묶인 브로커 중 일부에 장애가 발생하더라도 데이터를 유실하지 않고 안전하게 사용하기 위함
          카프카의 데이터 복제는 파티션 단위로 이루어짐, 토픽을 생성할 때 파티션의 복제 개수(replication factor)도 같이 설정되는데
          직접 옵션을 선택하지 않으면 브로커에 설정된 옵션값을 따라감
          복제 개수의 최솟값은 1(복제없음)이고 최댓값은 브로커 개수만큼 설정하여 사용할 수 있음

        • 복제된 파티션은 리더(leader)와 팔로워(follower)로 구성됨
          프로듀서 또는 컨슈머와 직접 통신하는 파티션을 리더, 나머지 복제 데이터를 가지고 있는 파티션을 팔로워라고 부름
          팔로워 파티션(파티션의 팔로워)들은 리더 파티션의 오프셋을 확인하여 현재 자신이 가지고 있는 오프셋과 차이가 나는 경우
          리더 파티션으로부터 데이터를 가져와서 자신의 파티션에 저장하는데, 이 과정을 복제라고 부름

        • 파티션 복제로 인해 나머지 브로커에도 파티션의 데이터가 복제되므로 복제 개수만큼의 저장 용량이 증가한다는 단점이 존재
          복제를 통해 데이터를 안전하게 사용할 수 있다는 강력한 장점 때문에 카프카를 운영할때 2개 이상의 복제 개수를 정하는 것이 중요함

        • 브로커에 장애가 발생한 경우
            • 브로커가 다운되면 해당 브로커에 있는 리더파티션은 사용할 수 없기 때문에 팔로워 파티션 중 하나가 리더 파티션 지위를 넘겨받는다
              이를 통해 데이터가 유실되지 않고 컨슈머나 프로듀서와 데이터를 주고받도록 동작 가능
              운영 시에는 데이터 종류마다 다른 복제 개수를 설정하고 상황에 따라서는 토픽마다 복제 개수를 다르게 설정하여 운영하기도 함
              데이터가 일부 유실되어도 무관하고 데이터 처리 속도가 중요하다면 1 또는 2로 설정 (metric)
              금융 정보와 같이 유실이 일어나면 안되는 데이터의 경우 복제개수를 3으로 설정하기도 함

        •ISR(In-Sync-Replicas)
            • ISR은 리더 파티션과 팔로워 파티션이 모두 싱크가 된 상태를 뜻함
              ex) 복제 개수가 2인 토픽을 가정해 볼 때, 토픽에는 리터 파티션 1개와 팔로워 파티션이 1개가 존재
                  리더 파티션에 0부터 3의 오프셋이 있다고 가정할 때, 팔로워 파티션에 동기화가 완료되려면 0부터 3까지 오프셋이 존재해야함
                  동기화가 완료됐다는 의미는 리더 파티션의 모든 데이터가 팔로워 파티션에 복제된 상태를 말하기 때문

            • unclean.leader.election.enable
                • 리더 파티션의 데이터를 모두 복제하지 못한 상태이고, 이렇게 싱크가 되지 않은 팔로워 파티션이 리더 파티션으로 선출되면 데이터가 유실될 수 있음
                  유실이 발생하더라도 서비스를 중단하지 않고 지속적으로 토픽을 사용하고 싶다면 ISR이 아닌 팔로워 파티션을 리더로 선출하도록 설정 가능
                  - unclean.leader.election.enable = true  : 유실을 감수함, 복제가 안된 팔로워 파티션을 리더로 승급
                  - unclean.leader.election.enable = false : 유실을 감수하지 않음, 해당 브로커가 복구될 때까지 중단

        • 하이 워터마크(High Watermark)
            • 리더 파티션과 팔로워 파티션에는 복제로 인해 각 파티션 당 저장되는 데이터의 개수가 달라진다
              이로 인해 복제 랙(replication lag)이 발생하는데, 컨슈머는 min.insync.recplias에 설정된 값 이상으로 복원된 레코드를 가져갈 수 있게됨
              이렇게 min.insync.replicas 개수 만큼 복제가 완료되어 컨슈머가 가져갈 수 있는 상태의 레코드의 오프셋 번호를 하이 워터마크라고 부름

• 토픽과 파티션
    • 토픽은 카프카에서 데이터를 구분하기 위해 사용하는 단위임, 토픽은 1개 이상의 파티션을 소유하고 있음
      파티션에는 프로듀서가 보낸 데이터들이 들어가 저장되는데 이 데이터를 '레코드(record)'라고 부름
      파티션은 자료구조에서 접하는 큐(queue)와 비슷한 구조라고 생각하면 쉬움, FIFO구조와 같이 먼저 들어간 레코드는 컨슈머가 먼저 가져감
      일반적인 자료구조로 사용되는 큐는 데이터를 가져가면(pop) 삭제하지만 카프카는 삭제하지 않음. 파티션의 레코드는 컨슈머가 가져가는 것과 별개로 관리됨
      이러한 특징 때문에 토픽의 레코드는 다양한 목적은 가진 여러 컨슈머 그룹들이 토픽의 데이터를 여러번 가져갈 수 있음

    • 파티션이 5개인 토픽을 생성했을 경우 0번 브로커부터 시작하여 round-robin 방식으로 리더 파티션들이 생성됨
      카프카 클라이언트는 리더 파티션이 있는 브로커와 통신하여 데이터를 주고 받으므로 여러 브로커에 골고루 네트워크 통신을 하게 됨
      이를 통해, 데이터가 특정 서버(여기서는 브로커)와 통신이 집중되는(hot spot) 형상을 막고 선형 확장(linear scale out)을 하여 데이터가 많아도 자연스럽게 대응가능
      * round-bin : 프로세스들 사이에 우선순위를 두지 않고, 순서대로 시간 단위로 CPU를 할당하는 방식의 CPU 스케줄링 알고리즘

    • 특정 브로커에 파티션이 몰리는 경우에는 kafka-reassign-partitions.oh 명령으로 파티션을 재분배할 수 있음

• 파티션 개수와 컨슈머 개수의 처리량 (파티션과 컨슈머의 관계는 1:1)
    • 파티션은 카프카의 병렬처리의 핵심으로써 그룹으로 묶인 컨슈머들이 레코드를 병렬로 처리할 수 있도록 매칭됨
      컨슈머의 처리량이 한정된 상황에서 많은 레코드를 병렬로 처리하는 가장 좋은 방법은 컨슈머의 개수를 늘려 스케일 아웃하는 방법
      * 컨슈머 개수를 늘림과 동시에 파티션 개수도 늘리면 처리량이 증가하는 효과를 봄
      컨슈머 랙(지연되는 상황)을 막기 위해 컨슈머 개수와 파티션 개수를 늘림으로써 대처

    • 파티션 개수를 줄이는건 불가능
        • 카프카에서 파티션 개수를 줄이는 것은 지원하지 않음, 파티션을 늘릴때 신중히 파티션 개수를 정해야함
          한번 늘리면 줄이는 것은 불가능 하기 때문에 토픽을 삭제하고 재생성하는 방법 외에는 없음
          카프카에서는 파티션의 데이터를 세그먼트로 저장하고 있으며 만에 하나 지원을 한다고 하더라도 여러 브로커에 저장된 데이터를 취합하고 정렬해야하는 과정을
          거쳐야 하기 때문에 클러스터에 큰 영향이 갈 수 있음 KIP-649에서 파티션 개수를 줄이는 것을 논의했지만 더이상 진행되지 않고 있음)

• 레코드
    • 레코드는 타임스탬프, 헤더, 메시지 키, 메시지 값, 오프셋으로 구성되어 있음
      프로듀서가 생성한 레코드가 브로커로 전송되면 오프셋과 타임스탬프가 지정되어 저장됨
      브로커에 한번 적재된 레코드는 수정할 수 없고 로그 리텐션 기간 또는 용량에 따라서만 삭제된다

    • 타임 스탬프
        • 레코드의 타임스탬프는 스트림 프로세싱에서 활용하기 위한 시간을 저장하는 용도로 사용됨, 카프카 0.10.0.0 이후 버전부터 추가된 Unix timestamp가 포함되며
          프로듀서에서 따로 설정하지 않으면 기본값으로 ProducerRecord 생성 시간(Create Time)이 들어감
          또는 브로커 적재 시간(LogAppendTime)으로 설정할 수 도 있으며, 해당 옵션은 토픽단위로 설정 가능하고 message.timestamp.type을 사용함

    • 오프셋
        • 레코드의 오프셋은 프로듀서가 생성한 레코드에는 존재하지 않음, 프로듀서가 전송한 레코드가 브로커에 적재될 때 오프셋이 지정됨
          오프셋은 0부터 시작되며 1씩 증가함
          컨슈머는 오프셋을 기반으로 처리가 완료된 데이터와 앞으로 처리해야할 데이터를 구분
          각 메시지는 파티션별로 고유한 오프셋을 가지므로 컨슈머에서 중복 처리를 방지하기 위한 목적으로도 사용함

    • 헤더
        • 레코드의 헤더는 0.11부터 제공된 기능으로 key/value 데이터를 추가할 수 있으며 레코드의 스키마 버전이나 포맷과 같이 데이터 프로세싱에 참고할만한 정보를 담아서 사용가능
          페이로드 같은게 아님

    • 메시지 키
        • 메시지 키는 처리하고자 하는 메시지 값을 분류하기 위한 용도로 사용되며, 이를 파티셔닝이라고 부름
          파티셔닝에 사용하는 메시지 키는 파티셔너(Partitioner)에 따라 토픽의 파티션 번호가 정해짐
          8메시지 키는 필수 값이 아니며, 지정하지 않으면 null로 설정됨
          메시지 키가 null인 레코드는 특정 토픽의 파티션에 round-robin으로 전달됨
          null이 아닌 메시지 키는 해쉬값에 의해서 특정 파티션에 매핑되어 전달됨(기본 파티셔너의 경우)

    • 메시지 값
        • 레코드의 메시지 값은 실질적으로 처리할 데이터가 담기는 공간임
          메시지 값의 포맷은 제네릭으로 사용자에 의해 지정됨
          Float, Byte[], String 등 다양한 형태로 지정 가능하며 필요에 따라 사용자 지정 포맷으로 직렬화/역직렬화 클래스를 만들어 사용 가능
          브로커에 저장된 레코드의 메시지 값은 어떤 포맷으로 직렬화되어 저장되어있는지 알 수 없기 때문에 컨슈머는 미리 역직렬화 포맷을 알고 있어야 함

•카프 러터
    • 카프카 클러스터가 연결된 주키퍼
        • 카프카 클러스터를 실행하기 위해서는 주키퍼(주키퍼 앙상블)가 필요함
          주키퍼의 서로 다른 znode에 클러스터를 지정하면 됨
          root znode에 각 클러스터별 znode를 생성하고 클러스터 실행시 root가 아닌 하위 znode로 설정
          카프카 3.0부터는 주키퍼가 없어도 클러스터 동작 가능
